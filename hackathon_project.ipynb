{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "!pip insall python-docx\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELLgjOuWl_XN",
        "outputId": "444e64d3-7335-4855-d28f-5a4ab981b8de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-GLzx0Xne1A",
        "outputId": "01fc2769-9b45-4550-c69d-97462a9f45a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoHUPikfvXx4",
        "outputId": "8f1c48d9-7975-43e0-ae69-4ba3264f5cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ashwini Kalidas Vedula\n",
            "Email ID: ashwini.vedula@nyu.edu | +1 (201)-616-1347 | LinkedIn: linkedin.com/in/ashwini-vedula\n",
            "Education\n",
            "New York University, New York, NY\n",
            "Sept 2021- May 2023\n",
            "Courant Institute of Mathematical Sciences and Stern School of Business\n",
            "Master of Science in Information Systems - 3.6/4\n",
            "Courses: Dealing with Data, Data Science for Business : Technical, Database Management Systems, Real Time\n",
            "Big Data Analytics, Cloud Computing\n",
            "Technical Skills\n",
            "Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB\n",
            "Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI\n",
            "Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure\n",
            "(Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive\n",
            "Experience\n",
            "Clark Associates Inc\n",
            "Pennsylvania, USA\n",
            "Financial Systems and Data Intern\n",
            "May 2022 - Aug 2022\n",
            "• Extracted data from varied data sources to automate Summary Income financial reports(monthly, quarterly and\n",
            "yearly) for the various subsidiaries of Clark Associates Inc. that reduced manual entry time by 70%.\n",
            "• Collaborated with cross-functional teams to enhance and optimize existing Power BI reports that helped stakeholders\n",
            "analyze the financial performance of their business.\n",
            "Dacapo Brokerage India Private Limited\n",
            "Mumbai,India\n",
            "Senior Analyst\n",
            "Aug 2020 - Jul 2021\n",
            "• Created visualization of daily top gainer/loser index stocks, relative sector performance and analyzed technical trends\n",
            "such as moving average of stock prices to help clients make informed investment decisions.\n",
            "CRISIL Limited, An S&P Global Company\n",
            "Mumbai,India\n",
            "Senior Associate - Data Science & Quants Lab\n",
            "Oct 2019 - Jul 2020\n",
            "• Lead a team of 2 interns to develop reports with functional specifications like KPI metrics, revenue tracking, budget\n",
            "vs. sales to understand opportunities in the sales funnel and improve operational effiency by 50% for the Senior\n",
            "Management of CRISIL’s Business Development Operations Team.\n",
            "Onsite Supply Chain Analytics Project - Doha, Qatar\n",
            "• Created and maintained 6 real time dashboards in Tableau that educated the Wholesale Banking Group on key\n",
            "metrics and performance measures.\n",
            "• Assisted in the design and development of Data Mart, fact tables using MySQL framework to decrease dependency\n",
            "on Tableau for complex calculations thereby reducing the dashboard loading time by 85%.\n",
            "• Exposure dealing with large relational data sets(8 Million+),load and query performance, archiving, stored\n",
            "procedures, etc.\n",
            "• Implemented a heuristic scoring model for approaching and on-boarding prospective customers that boosted\n",
            "conversions by 55%.\n",
            "• Built 31 cross-sell/up-sell rule-based configuration for product recommendations with Prescriptive Analytics.\n",
            "• Engaged with users in testing, release management, and operations to ensure quality of code development,\n",
            "deployment and post-production support.\n",
            "Associate Software Engineer - Data Science & Quants Lab\n",
            "Jul 2018 - Oct 2019\n",
            "• Determined sales forecasting for 250+ retainer stores of the biggest conglomerate in UAE using Holt-Winters\n",
            "algorithm.\n",
            "• Analysed data to develop a de-duplication algorithm model for grouping similar occurring brand names together.\n",
            "• Segmented 5 Million+ customers using RFM analysis to help the bank associates determine in advance the nature of\n",
            "future business operations and marketing strategies.\n",
            "• Performed Market Basket Analysis based on frequent purchasing patterns of customers.\n",
            "Leadership and Achievements\n",
            "• Conferred Quarterly Award,(Service Excellence Award - Q4 2019) at Crisil Limited.\n",
            "• Received CRISILite Award for Performance (CLAP), for the month of May 2019.\n",
            "• Treasurer of Computer Society of India-VESIT Student Chapter for the year 2016-2017.\n",
            "• Executive Head of Sponsorship Committee in the inter-collegiate technical festival.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# converting all format into text\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "import docx\n",
        "from bs4 import BeautifulSoup\n",
        "import subprocess\n",
        "import re\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Open the PDF file\n",
        "        pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "        # Iterate through each page\n",
        "        for page_number in range(len(pdf_document)):\n",
        "            # Get the page\n",
        "            page = pdf_document.load_page(page_number)\n",
        "\n",
        "            # Extract text from the page\n",
        "            page_text = page.get_text()\n",
        "\n",
        "            # Append page text to the result\n",
        "            text += page_text\n",
        "\n",
        "        # Close the PDF file\n",
        "        pdf_document.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Perform OCR on the image\n",
        "        text = pytesseract.image_to_string(image_path)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Open the DOCX file\n",
        "        doc = docx.Document(docx_path)\n",
        "\n",
        "        # Extract text from paragraphs\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + \"\\n\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_text_from_doc(doc_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        # Use antiword to extract text from .doc files\n",
        "        process = subprocess.Popen(['antiword', doc_path], stdout=subprocess.PIPE)\n",
        "        output, _ = process.communicate()\n",
        "        text = output.decode('utf-8')\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    file_extension = file_extension.lower()\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_extension == '.jpg' or file_extension == '.png':\n",
        "        return extract_text_from_image(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    elif file_extension == '.doc':\n",
        "        return extract_text_from_doc(file_path)\n",
        "    else:\n",
        "        print(\"Unsupported file format\")\n",
        "        return \"\"\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/drive/MyDrive/resumerevaler/documents20220826-1-v01cla.pdf\"\n",
        "extracted_text = extract_text_from_file(file_path)\n",
        "print(extracted_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting skill using regrex\n",
        "def parse_resume(text_data):\n",
        "    resume_data = {}\n",
        "\n",
        "    # Extracting personal information\n",
        "    personal_info_pattern = re.compile(r\"([^\\n]+)\\n\\s*([^\\n]+)\\s*\\|\\s*([^\\n]+)\\s*\\|\\s*([^\\n]+)\")\n",
        "    personal_info_match = personal_info_pattern.search(text_data)\n",
        "    if personal_info_match:\n",
        "        resume_data[\"Name\"] = personal_info_match.group(1).strip()\n",
        "        resume_data[\"Location\"] = personal_info_match.group(2).strip()\n",
        "        resume_data[\"Email\"] = personal_info_match.group(3).strip()\n",
        "        resume_data[\"Phone\"] = personal_info_match.group(4).strip()\n",
        "\n",
        "    # Extracting education\n",
        "    education_pattern = re.compile(r\"Education\\n([\\s\\S]+?)(?=Technical Skills|Skills|PROFESSIONAL EXPERIENCE|Experience|Achievements|$)\", re.IGNORECASE)\n",
        "    education_matches = education_pattern.findall(text_data)\n",
        "    if education_matches:\n",
        "        resume_data[\"Education\"] = [match.strip() for match in education_matches]\n",
        "    else:\n",
        "        # If there is no other section found after education, consider education until the end of the text\n",
        "        education_pattern = re.compile(r\"Education\\n([\\s\\S]+?)$\", re.IGNORECASE)\n",
        "        education_match = education_pattern.findall(text_data)\n",
        "        if education_match:\n",
        "            resume_data[\"Education\"] = [education_match[0].strip()]\n",
        "\n",
        "    # Extracting technical skills\n",
        "    skills_pattern = re.compile(r\"(Technical Skills|Skills)\\n([\\s\\S]+?)\\n(?=PROFESSIONAL EXPERIENCE|Experience|Achievements|$)\", re.IGNORECASE)\n",
        "    skills_match = skills_pattern.search(text_data)\n",
        "    if skills_match:\n",
        "        skills = skills_match.group(2).strip().split(\"\\n\")\n",
        "        if \"Technical Skills\" in skills_match.group(1):\n",
        "            resume_data[\"Skills\"] = \"\\n\".join(skills)\n",
        "        else:\n",
        "            resume_data[\"Additional Skills\"] = \"\\n\".join(skills)\n",
        "\n",
        "    # Extracting experience details\n",
        "    experience_pattern = re.compile(r\"(Experience|PROFESSIONAL EXPERIENCE)\\n([\\s\\S]+?)\\n(?=Education|Achievements|$)\", re.IGNORECASE)\n",
        "    experience_matches = experience_pattern.findall(text_data)\n",
        "    experience_list = []\n",
        "    for match in experience_matches:\n",
        "        company_info, responsibilities = match[0].strip(), match[1].strip()\n",
        "        experience_list.append(f\"{company_info}:\\n{responsibilities}\")\n",
        "    resume_data[\"Experience\"] = \"\\n\".join(experience_list)\n",
        "\n",
        "    # Extracting achievements\n",
        "    achievements_pattern = re.compile(r\"Achievements\\n([\\s\\S]+?)(?=\\n[^\\n]*:|$)\", re.IGNORECASE)\n",
        "    achievements_matches = achievements_pattern.findall(text_data)\n",
        "    resume_data[\"Achievements\"] = \", \".join([match.strip() for match in achievements_matches])\n",
        "    return resume_data\n",
        "\n",
        "def print_resume_details(resume_data):\n",
        "    for key, value in resume_data.items():\n",
        "        print(f\"{key}:\")\n",
        "        if isinstance(value, list):\n",
        "            for item in value:\n",
        "                print(f\"  - {item}\")\n",
        "        else:\n",
        "            print(f\"  {value}\")\n",
        "\n",
        "\n",
        "\n",
        "# Assuming you have the extracted text in a variable named extracted_text\n",
        "parsed_resume = parse_resume(extracted_text)\n",
        "print_resume_details(parsed_resume)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPJ18GH_ze4T",
        "outputId": "e52cd554-72bc-46fb-e747-82a72ca28cf9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:\n",
            "  Ashwini Kalidas Vedula\n",
            "Location:\n",
            "  Email ID: ashwini.vedula@nyu.edu\n",
            "Email:\n",
            "  +1 (201)-616-1347\n",
            "Phone:\n",
            "  LinkedIn: linkedin.com/in/ashwini-vedula\n",
            "Education:\n",
            "  - New York University, New York, NY\n",
            "Sept 2021- May 2023\n",
            "Courant Institute of Mathematical Sciences and Stern School of Business\n",
            "Master of Science in Information Systems - 3.6/4\n",
            "Courses: Dealing with Data, Data Science for Business : Technical, Database Management Systems, Real Time\n",
            "Big Data Analytics, Cloud Computing\n",
            "Skills:\n",
            "  Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB\n",
            "Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI\n",
            "Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure\n",
            "(Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive\n",
            "Experience:\n",
            "  Experience:\n",
            "Clark Associates Inc\n",
            "Pennsylvania, USA\n",
            "Financial Systems and Data Intern\n",
            "May 2022 - Aug 2022\n",
            "• Extracted data from varied data sources to automate Summary Income financial reports(monthly, quarterly and\n",
            "yearly) for the various subsidiaries of Clark Associates Inc. that reduced manual entry time by 70%.\n",
            "• Collaborated with cross-functional teams to enhance and optimize existing Power BI reports that helped stakeholders\n",
            "analyze the financial performance of their business.\n",
            "Dacapo Brokerage India Private Limited\n",
            "Mumbai,India\n",
            "Senior Analyst\n",
            "Aug 2020 - Jul 2021\n",
            "• Created visualization of daily top gainer/loser index stocks, relative sector performance and analyzed technical trends\n",
            "such as moving average of stock prices to help clients make informed investment decisions.\n",
            "CRISIL Limited, An S&P Global Company\n",
            "Mumbai,India\n",
            "Senior Associate - Data Science & Quants Lab\n",
            "Oct 2019 - Jul 2020\n",
            "• Lead a team of 2 interns to develop reports with functional specifications like KPI metrics, revenue tracking, budget\n",
            "vs. sales to understand opportunities in the sales funnel and improve operational effiency by 50% for the Senior\n",
            "Management of CRISIL’s Business Development Operations Team.\n",
            "Onsite Supply Chain Analytics Project - Doha, Qatar\n",
            "• Created and maintained 6 real time dashboards in Tableau that educated the Wholesale Banking Group on key\n",
            "metrics and performance measures.\n",
            "• Assisted in the design and development of Data Mart, fact tables using MySQL framework to decrease dependency\n",
            "on Tableau for complex calculations thereby reducing the dashboard loading time by 85%.\n",
            "• Exposure dealing with large relational data sets(8 Million+),load and query performance, archiving, stored\n",
            "procedures, etc.\n",
            "• Implemented a heuristic scoring model for approaching and on-boarding prospective customers that boosted\n",
            "conversions by 55%.\n",
            "• Built 31 cross-sell/up-sell rule-based configuration for product recommendations with Prescriptive Analytics.\n",
            "• Engaged with users in testing, release management, and operations to ensure quality of code development,\n",
            "deployment and post-production support.\n",
            "Associate Software Engineer - Data Science & Quants Lab\n",
            "Jul 2018 - Oct 2019\n",
            "• Determined sales forecasting for 250+ retainer stores of the biggest conglomerate in UAE using Holt-Winters\n",
            "algorithm.\n",
            "• Analysed data to develop a de-duplication algorithm model for grouping similar occurring brand names together.\n",
            "• Segmented 5 Million+ customers using RFM analysis to help the bank associates determine in advance the nature of\n",
            "future business operations and marketing strategies.\n",
            "• Performed Market Basket Analysis based on frequent purchasing patterns of customers.\n",
            "Leadership and Achievements\n",
            "• Conferred Quarterly Award,(Service Excellence Award - Q4 2019) at Crisil Limited.\n",
            "• Received CRISILite Award for Performance (CLAP), for the month of May 2019.\n",
            "• Treasurer of Computer Society of India-VESIT Student Chapter for the year 2016-2017.\n",
            "• Executive Head of Sponsorship Committee in the inter-collegiate technical festival.\n",
            "Achievements:\n",
            "  • Conferred Quarterly Award,(Service Excellence Award - Q4 2019) at Crisil Limited.\n",
            "• Received CRISILite Award for Performance (CLAP), for the month of May 2019.\n",
            "• Treasurer of Computer Society of India-VESIT Student Chapter for the year 2016-2017.\n",
            "• Executive Head of Sponsorship Committee in the inter-collegiate technical festival.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_resume_details(resume_data):\n",
        "    for key, value in resume_data.items():\n",
        "        print(f\"{key}:\")\n",
        "        if isinstance(value, list):\n",
        "            for item in value:\n",
        "                print(f\"  - {item}\")\n",
        "        else:\n",
        "            print(f\"  {value}\")\n",
        "\n",
        "def extract_skills(resume_data):\n",
        "    all_skills = []\n",
        "\n",
        "    # Regular expression to match skills and sub-skills\n",
        "    skill_pattern = re.compile(r\"([^\\n:]+):?\")  # Matches any characters except newline and colon, optionally followed by a colon\n",
        "\n",
        "    if \"Skills\" in resume_data:\n",
        "        skills_match = skill_pattern.findall(resume_data[\"Skills\"])\n",
        "        for skill in skills_match:\n",
        "            all_skills.extend([sub_skill.strip() for sub_skill in re.split(r'[;,]', skill)])\n",
        "\n",
        "    if \"Additional Skills\" in resume_data:\n",
        "        additional_skills_match = skill_pattern.findall(resume_data[\"Additional Skills\"])\n",
        "        for skill in additional_skills_match:\n",
        "            all_skills.extend([sub_skill.strip() for sub_skill in re.split(r'[;,]', skill)])\n",
        "\n",
        "    return all_skills\n",
        "\n",
        "\n",
        "print(\"\\nExtracted Skills:\")\n",
        "skills = extract_skills(parsed_resume)\n",
        "for skill in skills:\n",
        "    print(\"  -\", skill)\n",
        "\n"
      ],
      "metadata": {
        "id": "1lP9LzE7fqMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb0be407-d57b-4953-e64a-59fc7a025ea2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Skills:\n",
            "  - Databases & Tools\n",
            "  - MySQL\n",
            "  - SSMS\n",
            "  - MS Access\n",
            "  - PostgreSQL\n",
            "  - PL/SQL\n",
            "  - MongoDB\n",
            "  - Data Analysis & Visualization Tools\n",
            "  - Python\n",
            "  - R\n",
            "  - SQL\n",
            "  - MS Excel\n",
            "  - Tableau\n",
            "  - Power BI\n",
            "  - Cloud Platforms & Big Data Technologies\n",
            "  - AWS (Redshift\n",
            "  - EC2\n",
            "  - S3\n",
            "  - EMR\n",
            "  - SageMaker\n",
            "  - DynamoDB Table)\n",
            "  - Azure\n",
            "  - (Storage\n",
            "  - IoT\n",
            "  - Virtual Machines)\n",
            "  - GCP (BigTable\n",
            "  - DataLab)\n",
            "  - Apache Hadoop\n",
            "  - Hive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#skill extraction using dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def extract_skills_from_resume(text, skills_file):\n",
        "    # Load skills list from Excel file\n",
        "    skills_df = pd.read_excel(skills_file)\n",
        "\n",
        "    # Remove leading/trailing whitespace from skills\n",
        "    skills_df['Category'] = skills_df['Category'].str.strip()\n",
        "\n",
        "    # Initialize list to store matched skills\n",
        "    matched_skills = set()\n",
        "\n",
        "    # Iterate through each skill in the list\n",
        "    for skill in skills_df['Category']:\n",
        "        # Create a regex pattern for the skill (escaping special characters)\n",
        "        pattern = r\"\\b{}\\b\".format(re.escape(skill))\n",
        "\n",
        "        # Search for the skill in the resume text, ignoring case\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "\n",
        "        # If a match is found, add the skill to the list of matched skills\n",
        "        if match:\n",
        "            matched_skills.add(skill)\n",
        "\n",
        "    return matched_skills\n",
        "\n",
        "# Path to the Excel file containing technology skills list\n",
        "skills_file_path = \"/content/drive/MyDrive/resumerevaler/csv/Technology_Skills_Competencies.xlsx\"\n",
        "\n",
        "# Extract skills from the resume text based on the provided skills list\n",
        "matched_skills = extract_skills_from_resume(extracted_text, skills_file_path)\n",
        "\n",
        "# Print the matched skills\n",
        "print(\"Matched Skills:\", matched_skills)\n"
      ],
      "metadata": {
        "id": "U9YhqjQvQUFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7250b1-0a95-45b5-ae4e-f9c345bd3b42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched Skills: {'Database management systems', 'MongoDB', 'Software', 'Apache Hadoop', 'Python', 'PostgreSQL', 'MySQL', 'R', 'LinkedIn', 'Tableau'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#occupation\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def extract_occupations_from_resume(text, occupations_file):\n",
        "    # Load occupations list from Excel file\n",
        "    occupations_df = pd.read_excel(occupations_file)\n",
        "\n",
        "    # Remove leading/trailing whitespace from occupations\n",
        "    occupations_df['Title'] = occupations_df['Title'].str.strip()\n",
        "\n",
        "    # Initialize set to store matched occupations\n",
        "    matched_occupations = set()\n",
        "\n",
        "    # Iterate through each occupation in the list\n",
        "    for occupation in occupations_df['Title']:\n",
        "        # Create a regex pattern for the occupation (escaping special characters)\n",
        "        pattern = r\"\\b{}\\b\".format(re.escape(occupation))\n",
        "\n",
        "        # Search for the occupation in the resume text, ignoring case\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "\n",
        "        # If a match is found, add the occupation to the set of matched occupations\n",
        "        if match:\n",
        "            matched_occupations.add(occupation)\n",
        "\n",
        "    return matched_occupations\n",
        "\n",
        "# Path to the Excel file containing occupations list\n",
        "occupations_file_path = \"/content/drive/MyDrive/resumerevaler/csv/OccupationData.xlsx\"\n",
        "\n",
        "# Extract occupations from the resume text based on the provided occupations list\n",
        "matched_occupations = extract_occupations_from_resume(extracted_text, occupations_file_path)\n",
        "\n",
        "# Print the matched occupations\n",
        "print(\"Matched Occupations:\", matched_occupations)\n"
      ],
      "metadata": {
        "id": "rjip4SrBuAkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8985639c-8328-40a8-bba6-3449ad29c163"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matched Occupations: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6buuIKojEJdr"
      }
    }
  ]
}